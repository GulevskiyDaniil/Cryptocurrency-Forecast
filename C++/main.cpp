#include <iostream>
#include <vector>
#include <algorithm>
#include <Eigen/Eigen> // <Eigen/Dense> for dense only, <Eigen/Sparse> for sparse only, <Eigen/Eigen> for both

// TUTORIALS                http://eigen.tuxfamily.org/dox/group__DenseMatrixManipulation__chapter.html
// DENSE TUTORIAL:          http://eigen.tuxfamily.org/dox/group__TutorialMatClass.html
// SPARSE TUTORIAL:         http://eigen.tuxfamily.org/dox/group__TutorialSparse.html
// DENSE QUICK REFERENCE:   http://eigen.tuxfamily.org/dox/group__QuickRefPage.html
// SPARSE QUICK REFERENCE:  http://eigen.tuxfamily.org/dox/group__SparseQuickRefPage.html


typedef Eigen::MatrixXd Mat;
typedef Eigen::VectorXd Vec;
typedef Eigen::ArrayXXd Arr; // Note the double X for arrays, unlike the single X for matrices!
typedef Eigen::SparseMatrix<double> SpMat;

void print_vector(std::vector<int> v) {
    std::cout << std::endl;
    for (std::vector<int>::const_iterator i = v.begin(); i != v.end(); ++i)
        std::cout << *i << ' ';
    std::cout << std::endl;
}


// http://www.quantdec.com/misc/MAT8406/Meeting12/SVD.pdf page 3
Vec Ridge(const Mat& X, const Vec& y, double alpha) {
    /*
    Solves ridge regression problem with object-feature matrix X, target y and l2 regularization coeff alpha
    */
    Eigen::BDCSVD<Mat> svd(X, Eigen::ComputeThinU | Eigen::ComputeThinV);
    Vec singular_values = svd.singularValues();
    Mat diag = (1.0 / (singular_values.array().square() + alpha) * singular_values.array()).matrix().asDiagonal();
    return svd.matrixV() * diag * svd.matrixU().transpose() * y;
}

// http://www.cs.utexas.edu/~cjhsieh/icdm-pmf.pdf Section IIIA
void FStep(const Mat& Y, const Mat& X, const Arr& omega, Mat& F, double lambda, double epsilon, int max_iter) {
    /*
    Solves problem ||P_omega(Y - F * X)|| + lambda * ||F|| --F--> min using coordinate descent algorithm
    omega is an array with same shape as Y with ones where Y_ij is observed and zeros where Y_ij is missing.
    P_omega is the projector in the space of matrices onto the subspace generated by observed indices.
    The iterative process is stopped if the sum of squares of changes of elements of F is less than epsilon.
    The problem is solved inplace, so F is overwritten with the answer.
    */
    Mat R = Y - F * X;
    for(int i=0; i < max_iter; ++i) {
        double delta_sq = 0;
        for(int row = 0; row < F.rows(); ++row) {
            for(int col = 0; col < F.cols(); ++col) {
                double denom = lambda + (X.row(col).array() * X.row(col).array() * omega.row(row).array()).matrix().sum();
                double numer = (X.row(col).array() * (R.row(row).array() + F(row, col) * X.row(col).array()) * omega.row(row).array()).matrix().sum();
                double f = numer/denom;
                R.row(row).array() -= (f - F(row, col)) * X.row(col).array();
                delta_sq += (f - F(row, col)) * (f - F(row, col));
                F(row, col) = f;
            }
        }
        if(delta_sq < epsilon) {
            return;
        }
    }
}

// https://en.wikipedia.org/wiki/Conjugate_gradient_method#The_resulting_algorithm
void ConjugateGradient(const Mat& A, const Vec& b, Vec& x, double epsilon){
    /*
    Solves Ax = b for symmetric positive definite A. Parameter x is a first approximation for the solution, if in doubt use random.
    The problem is solved inplace, so x is overwritten with the answer.
    */
    Vec r0 = b - A * x;
    Vec r1 = r0;
    Vec p = r0;
    for (int k=0; k<A.rows(); ++k) {
        double alpha = r0.dot(r0) / (p.transpose() *  A * p);
        x = x + alpha * p;
        r1 = r0 - alpha * A * p;
        if (r1.norm() < epsilon) {
            return;
        }
        double beta = r1.dot(r1) / r0.dot(r0);
        p = r1 + beta * p;
        r0 = r1;
    }
    return;
}

// https://en.wikipedia.org/wiki/Conjugate_gradient_method#The_resulting_algorithm
void SparseConjugateGradient(const SpMat& A, const Vec& b, Vec& x, double epsilon){
    /*
    Solves AX=b for sparse symmetric positive definite A.
    Only the upper half of A shouls be filled, the lower part will be ignored.
    */
    Vec r0 = b - A.selfadjointView<Eigen::Upper>() * x;
    Vec r1 = r0;
    Vec p = r0;
    for (int k=0; k<A.rows(); ++k) {
        double alpha = r0.dot(r0) / (p.transpose() *  A.selfadjointView<Eigen::Upper>() * p);
        x = x + alpha * p;
        r1 = r0 - A.selfadjointView<Eigen::Upper>() * p * alpha;
        if (r1.norm() < epsilon) {
            return;
        }
        double beta = r1.dot(r1) / r0.dot(r0);
        p = r1 + beta * p;
        r0 = r1;
    }
    return;
}


void WStep(const Mat& X, Mat& W, std::vector<int> lags, double lambda_w, double lambda_x) {
    /*
    Solves for autoregression weights with given matrix X. All this function does is initialize matrices and vectors for ridge regression
    */
    std::sort(lags.begin(), lags.end());
    int T = X.cols();
    int k = X.rows();
    int l = lags.size();
    int L = lags.back();
    double lambda = 0.5 * lambda_w / lambda_x;

    for (int row =  0; row < k; ++row) {
        Mat M(T - L, l);
        Vec y(T - L);
        for (int i=L; i<T; ++i) {
            y(i - L) = X(row, i);
            for(int j=0; j<l; ++j) {
                M(i - L, l - 1 -j) = X(row, i - lags[j]);
            }
        }
        W.row(row) = Ridge(M, y, lambda).transpose();
    }
}

int main() {
    // Test ridge regression

    std::cout << std::endl << "Test ridge regression" << std::endl;
    Mat M(7,3);
    M << 3,5,7,
         4,6,8,
         4,4,7,
         2,4,6,
         4,7,4,
         7,4,7,
         9,5,3;

    Vec y(7);
    y << 15, 18, 15, 12, 15, 18, 17;

    double alpha = 0.01;

    Vec answer = Ridge(M, y, alpha);
    std::cout << std::endl << answer << std::endl;

    std::cout << answer.rows() << std::endl;

    // Test coordinate descent: generate F, X , let Y = FX, pretend you dont know true F,
    // solve Y=FX for F with lambda = 0, should get exactly F

    std::cout << std::endl << "Test coordinate descent" << std::endl;

    Arr omega(4, 10); // 1 if known, 0 if missiing
    omega << 0,1,1,1,0,0,1,1,1,1,
             1,1,0,1,1,1,0,1,1,1,
             1,1,1,1,1,1,1,1,1,1,
             0,1,1,1,1,1,1,0,1,1;

    Mat X(2,10);
    X << 4,8,2,6,0,3,4,1,5,7,
         0,5,8,3,2,5,7,9,4,8;

    Mat F(4, 2);
    F << 4,2,
         3,1,
         6,2,
         0,2;

    Mat Y = (F * X).array() * omega;

    Mat F_prime(4, 2);
    F_prime.setRandom();

    std::cout << "Starting point for F" << std::endl << F_prime << std::endl;

    double lambda_f = 0.0;
    double epsilon = 0.00000001; // tolerance for coordinate descent
    int max_iter = 1000;

    FStep(Y, X, omega, F_prime, lambda_f, epsilon, max_iter);

    std::cout << "True F" << std::endl << F << std::endl;
    std::cout << "Recovered F" << std::endl << F_prime << std::endl;

    std::cout << std::endl << "Test conjugate gradient" << std::endl;

    Mat A(4,4);
    A << 0.277662, 0.296741, 0.0138198, 0.564052,
         0.296741, 1.58477, -0.531986, 0.0980013,
         0.0138198, -0.531986, 0.822107, 0.164156,
         0.564052, 0.0980013, 0.164156, 2.09679;

    Vec b(4);
    b << -0.686642, -0.198111, -0.740419, -0.782382;
    Vec x(4);
    x.setRandom();
    std::cout << "Matrix A:" << std::endl << A << std::endl;
    std::cout << "Vector b:" << std::endl << b << std::endl;
    std::cout << "Vector x:" << std::endl << x << std::endl;

    ConjugateGradient(A, b, x, epsilon);

    std::cout<< std::endl << "Answer:" << std::endl << x << std::endl;

    std::cout << std::endl << "Test sparse conjugate gradient" << std::endl;

    SpMat MS(4, 4);

    for(int i=0; i<4; ++i) {
        MS.insert(i,i) = i + 1;
    }

    MS.insert(2,3) = 0.4;
    MS.insert(0,2) = -0.2;
    MS.insert(1,2) = 0.6;
    // USE SELF ADJOINT VIEW!!! Initialize only upper half of matrix
    std::cout << "Sparse matrix MS, upper self-adjoint view:" << std::endl << MS.selfadjointView<Eigen::Upper>() << std::endl;
    std::cout << "Vector b:" << std::endl << b << std::endl;
    std::cout << "Vector x:" << std::endl << x << std::endl;

    SparseConjugateGradient(MS, b, x, epsilon);
    std::cout<< std::endl << "Answer:" << std::endl << x << std::endl;

    ///

    std::vector<int> lags;
    lags.push_back(1);
    lags.push_back(7);
    lags.push_back(3);

    std::cout << "Lags: " << std::endl;
    print_vector(lags);

    Mat W(X.rows(), lags.size());
    W.setRandom();

    std::cout << std::endl << W << std::endl;
    double lambda_w = 0.1;
    double lambda_x = 0.5;
    WStep(X, W, lags, lambda_w, lambda_x);
    std::cout << std::endl << W << std::endl;

    return 0;
}
